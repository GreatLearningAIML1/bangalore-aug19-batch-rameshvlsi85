{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0lV6O-EKlbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f24df21-798b-4a96-cbee-0f8bc80123ff"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVnNcq6KvKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53d53b83-85fb-4aa2-8014-f3ce99e77d32"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36f5f93c-598b-479c-9cf6-55be66360ae8"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZI2gfVeQijE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as py\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of elelements in numpy shape.\n",
        "# no. images height and width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5e4aef6-c740-4976-f46a-2516d68e3b19"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAF1FpTdOMjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f04c9043-99c0-46b1-a017-f70cf8967d3b"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAPLGBaPOMm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f38a398c-54a4-444f-d659-25440a681c22"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfAXELvFOd8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc7ee614-7696-492d-d244-1c6a35fa006c"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34955659-5c96-4c4e-da84-77c899982b07"
      },
      "source": [
        "x_train.ndim"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding so that CNN doesnot learn lables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgHSCXy3JjwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e4ac212-d855-4483-a339-0fc39202eda6"
      },
      "source": [
        "np.unique(y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbf28vREQ3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = 10, dtype = \"float32\")\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes = 10 , dtype = \"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lKWJ_P9R3zN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac8db7d8-0bf5-45f1-e1e1-1c9fb641d0c5"
      },
      "source": [
        " print(\"Shape of data (AFTER encode) for y_train %s and for y_test %s\" % (str(y_train.shape), str(y_test.shape)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data (AFTER encode) for y_train (60000, 10) and for y_test (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2aaf22b9-81fb-4c4b-9023-1900bc05609e"
      },
      "source": [
        "print(x_train.dtype, x_test.dtype)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uint8 uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXsRKgajT1Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test.astype(\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuduNdOxUXgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype(\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgiLdZOyUcSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b583940-364b-49f7-d3f6-d99b764adbb8"
      },
      "source": [
        "print(x_train.dtype, x_test.dtype)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32 float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnudnQ0TUd4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pixel 0- 255 to be normalised divide by /255\n",
        "x_test = x_test/255\n",
        "x_train = x_train/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wixv7UY4U7Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "af3160a8-e968-41c0-c486-28f7a60aad92"
      },
      "source": [
        "print (x_test.shape)\n",
        "print (x_train.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28)\n",
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolution layer in keras expect no. of images and dimension 28x28 - gray scale (28x28x1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psIA1J5WVM5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDH_bBstV6p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8c437871-7e40-4548-91c7-d55571077217"
      },
      "source": [
        "print('--- THE DATA ---')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- THE DATA ---\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3x3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sequention model \n",
        "#convolution layer 3x3\n",
        "#\n",
        "#second flatten \n",
        "# optimiser \n",
        "# early stoping "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNllWtFSZwLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model\n",
        "    model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MKaXTZIZxdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # 1st Conv Layer\n",
        "    model.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1))) # 32 filters of 3x3\n",
        "    model.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH0JxzzMa9Ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # 2nd Conv Layer\n",
        "    model.add(Convolution2D(32, 3, 3))   \n",
        "    model.add(Activation('relu'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCgcBNglcMWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Fully Connected Layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128))\n",
        "    model.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju0XBY9Xc2ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Prediction Layer\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuYE9V6Uc-Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Loss and Optimizer\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lelpQ9tMdRAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Store Training Results\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9B4rEhxdhSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32 # defualt batch size\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wEXRfnpdqQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "03b63561-9f98-4836-9b94-94ddace6ebb1"
      },
      "source": [
        "   # Train the model\n",
        "    model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "59584/60000 [============================>.] - ETA: 0s - loss: 0.5036 - accuracy: 0.8211WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 9s 150us/sample - loss: 0.5026 - accuracy: 0.8214 - val_loss: 0.4069 - val_accuracy: 0.8556\n",
            "Epoch 2/10\n",
            "59584/60000 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8717WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3497 - accuracy: 0.8716 - val_loss: 0.3627 - val_accuracy: 0.8669\n",
            "Epoch 3/10\n",
            "59520/60000 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8872WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3057 - accuracy: 0.8871 - val_loss: 0.3239 - val_accuracy: 0.8807\n",
            "Epoch 4/10\n",
            "59584/60000 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.8967WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.2772 - accuracy: 0.8969 - val_loss: 0.3097 - val_accuracy: 0.8841\n",
            "Epoch 5/10\n",
            "59776/60000 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.9040WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.2581 - accuracy: 0.9041 - val_loss: 0.3575 - val_accuracy: 0.8696\n",
            "Epoch 6/10\n",
            "59648/60000 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.9092WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.2427 - accuracy: 0.9092 - val_loss: 0.2908 - val_accuracy: 0.8947\n",
            "Epoch 7/10\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9144WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.2254 - accuracy: 0.9143 - val_loss: 0.2972 - val_accuracy: 0.8953\n",
            "Epoch 8/10\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9198WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.2135 - accuracy: 0.9197 - val_loss: 0.2936 - val_accuracy: 0.8945\n",
            "Epoch 9/10\n",
            "59520/60000 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9245WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.2002 - accuracy: 0.9244 - val_loss: 0.2825 - val_accuracy: 0.8999\n",
            "Epoch 10/10\n",
            "59520/60000 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9282WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.1884 - accuracy: 0.9283 - val_loss: 0.2817 - val_accuracy: 0.8999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f110012fe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEMAS9bEZJwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# second model add maxpool and dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epytn1YBe5zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Define Model\n",
        "    model2 = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W_iNsWsgvvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # 1st Conv Layer\n",
        "    model2.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model2.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model2.add(Convolution2D(32, 3, 3))\n",
        "    model2.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFrc7sjTg1Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Max Pooling\n",
        "    model2.add(MaxPooling2D(pool_size=(2,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nySF8fwg5eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Dropout\n",
        "    model2.add(Dropout(0.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTeuf6Hhch4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Fully Connected Layer\n",
        "    model2.add(Flatten())\n",
        "    model2.add(Dense(128))\n",
        "    model2.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FtKaErDhjjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Prediction Layer\n",
        "    model2.add(Dense(10))\n",
        "    model2.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwSkM-z2hpGc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "2a63e2c1-4edb-4060-e5a4-c03001499ae8"
      },
      "source": [
        "  # Loss and Optimizer\n",
        "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Store Training Results\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n",
        "\n",
        "    # Train the model\n",
        "    model2.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "59520/60000 [============================>.] - ETA: 0s - loss: 0.9413 - accuracy: 0.6596WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 8s 126us/sample - loss: 0.9396 - accuracy: 0.6603 - val_loss: 0.6387 - val_accuracy: 0.7678\n",
            "Epoch 2/10\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.6938 - accuracy: 0.7469WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.6935 - accuracy: 0.7469 - val_loss: 0.5974 - val_accuracy: 0.7834\n",
            "Epoch 3/10\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.6373 - accuracy: 0.7681WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.6371 - accuracy: 0.7681 - val_loss: 0.5591 - val_accuracy: 0.7959\n",
            "Epoch 4/10\n",
            "59776/60000 [============================>.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7784WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.6005 - accuracy: 0.7783 - val_loss: 0.5378 - val_accuracy: 0.8023\n",
            "Epoch 5/10\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.5787 - accuracy: 0.7863WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.5785 - accuracy: 0.7864 - val_loss: 0.5207 - val_accuracy: 0.8060\n",
            "Epoch 6/10\n",
            "59744/60000 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7916WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.5619 - accuracy: 0.7914 - val_loss: 0.5167 - val_accuracy: 0.8140\n",
            "Epoch 7/10\n",
            "59808/60000 [============================>.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7967WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5498 - accuracy: 0.7968 - val_loss: 0.5019 - val_accuracy: 0.8150\n",
            "Epoch 8/10\n",
            "59776/60000 [============================>.] - ETA: 0s - loss: 0.5402 - accuracy: 0.7994WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.5406 - accuracy: 0.7991 - val_loss: 0.4981 - val_accuracy: 0.8167\n",
            "Epoch 9/10\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.5299 - accuracy: 0.8031WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.5296 - accuracy: 0.8031 - val_loss: 0.4964 - val_accuracy: 0.8188\n",
            "Epoch 10/10\n",
            "59584/60000 [============================>.] - ETA: 0s - loss: 0.5218 - accuracy: 0.8053WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.5211 - accuracy: 0.8057 - val_loss: 0.4845 - val_accuracy: 0.8181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f10e8418ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivygVptjpw9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## import imageDataDenerator control how much rotation with different parameter(default value)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  ## preprocessing library It has image data generator\n",
        "                                                          # Data augmentation - ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(horizontal_flip= True, vertical_flip = True, rotation_range=50)\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "c19fee15-649d-4529-efe1-fc90267c6b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nReRLN5FIJLqIfOkmEolEF/F/u2s+aLqsykoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmtTBayCL-aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# python generators on runtime to save memory(Datagen)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator object as input during the training time."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLfv1yLmsHxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "f38f83d2-b9cb-47dd-a003-c8a0f6069453"
      },
      "source": [
        "model2.fit_generator(datagen.flow(x_train, y_train,batch_size=32),   # for every images\n",
        "                    steps_per_epoch=x_train.shape[0]/32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_test, y_test), callbacks=callback_list)   # check points to store models backup of weights after epochs. "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-48-1030e57373c2>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 1875.0 steps, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 1.3522 - accuracy: 0.5002WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.3512 - accuracy: 0.5005 - val_loss: 0.8215 - val_accuracy: 0.7106\n",
            "Epoch 2/10\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 1.1762 - accuracy: 0.5623WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.1763 - accuracy: 0.5623 - val_loss: 0.8416 - val_accuracy: 0.7020\n",
            "Epoch 3/10\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 1.1115 - accuracy: 0.5873WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.1117 - accuracy: 0.5873 - val_loss: 0.8590 - val_accuracy: 0.6890\n",
            "Epoch 4/10\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 1.0801 - accuracy: 0.5995WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.0800 - accuracy: 0.5995 - val_loss: 0.8430 - val_accuracy: 0.6963\n",
            "Epoch 5/10\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 1.0492 - accuracy: 0.6125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.0489 - accuracy: 0.6126 - val_loss: 0.8663 - val_accuracy: 0.6824\n",
            "Epoch 6/10\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.6172WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.0297 - accuracy: 0.6172 - val_loss: 0.8550 - val_accuracy: 0.6960\n",
            "Epoch 7/10\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 1.0076 - accuracy: 0.6287WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.0077 - accuracy: 0.6287 - val_loss: 0.8352 - val_accuracy: 0.7046\n",
            "Epoch 8/10\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 1.0062 - accuracy: 0.6284WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 1.0060 - accuracy: 0.6285 - val_loss: 0.8403 - val_accuracy: 0.6961\n",
            "Epoch 9/10\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.9915 - accuracy: 0.6337WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.9915 - accuracy: 0.6335 - val_loss: 0.8418 - val_accuracy: 0.6850\n",
            "Epoch 10/10\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.9823 - accuracy: 0.6388WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.9822 - accuracy: 0.6389 - val_loss: 0.8232 - val_accuracy: 0.6930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f10e815e908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsqPZ72Mledw",
        "colab_type": "code",
        "outputId": "cb7fe006-ef1f-46fe-bd52-22f201a1e1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "loss_and_metrics = model2.evaluate(x_train, y_train)\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7974 - accuracy: 0.7037\n",
            "[0.7974332422097524, 0.7036833]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train1, y_train1), (x_test1, y_test1) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image shifting different parameters \n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip= True, vertical_flip = True, rotation_range=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the generator\n",
        "datagen.fit(x_train1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "56de0eea-227b-4090-957a-ab030595e10d"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train1[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze().astype('uint8'), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}